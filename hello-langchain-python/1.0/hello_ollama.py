from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM
import sys
from hello_utils import clean_think_sections

def main():
    template = """ä½ æ˜¯é¡¶çº§çš„çŸ­ç‰‡ä½œå®¶ï¼Œ
    ä½¿ç”¨{lang},è¯·æ ¹æ®{title}çš„å†…å®¹ï¼Œå†™ä¸€ç¯‡50å­—çš„ç²¾å“çŸ­æ–‡"""
    prompt = ChatPromptTemplate.from_template(template)

    model_name = "qwen3:8b"
    title = "çª—å¤–"
    languages = ["è‹±è¯­", "æ³•è¯­", "ä¿„è¯­", "æ±‰è¯­"]
    
    try:
        # Initialize model with more explicit parameters
        llama_model = OllamaLLM(
            model=model_name,
            base_url="http://localhost:11434",
            temperature=0,
            # Set a reasonable timeout
        )
        
        chain = prompt | llama_model
        
        print(f"\nğŸš€ è¿è¡Œæ¨¡å‹: {model_name}")
        for lang in languages:
            print(f"\n===== {lang} =====")
            try:
                result = chain.invoke({"title": title, "lang": lang})
                clean_result = clean_think_sections(result)
                print(clean_result)
            except Exception as e:
                print(f"å¤„ç†{lang}æ—¶å‡ºé”™: {str(e)}")
                
    except KeyboardInterrupt:
        print("\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nç¨‹åºé‡åˆ°é”™è¯¯: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
